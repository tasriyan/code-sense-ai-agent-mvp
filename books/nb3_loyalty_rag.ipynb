{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### What we are testing here:\n",
    "- Basic Semantic Retrieval Testing: Query testing with loyalty-specific queries\n",
    "- Distance analysis for semantic similarity quality\n",
    "- Result ranking and relevance assessment\n",
    "- File and project coverage analysis\n",
    "- Filtered Retrieval Testing: semantic + filtering by project type, or file type, or project name\n",
    "- Edge cases: malformed queries, empty queries, very long queries, etc.\n",
    "    Examples:\n",
    "    - Basic queries (loyalty points microservice specific):\n",
    "        - \"loyalty points calculation rules\"\n",
    "        - \"order processing workflow\"\n",
    "        - \"customer data integration\"\n",
    "        - \"payment service integration\"\n",
    "        - \"business rule patterns\"\n",
    "    - Filtered Queries:\n",
    "        - C# files only\n",
    "        - Configuration files only\n",
    "        - Specific project filtering\n",
    "#### Reporting:\n",
    "- Human readable reports to use to compare embedding models and classifier performance\n",
    "For example, the reports can be used for this type of analysis:\n",
    "##### Updated Performance Analysis\n",
    "\n",
    "| Embedding Model | LLM | Avg Distance | Query Coverage | Best Query Distance |\n",
    "|----------------|-----|-------------|----------------|-------------------|\n",
    "| **all-MiniLM-L6-v2** | **GPT-4.1** | **1.1287** | 54.55% | **0.6485** |\n",
    "| all-MiniLM-L6-v2 | CodeLlama | 1.1854 | **63.64%** | 0.7035 |\n",
    "| all-MiniLM-L6-v2 | Claude 3.5 | 1.1859 | 42.42% | 0.8229 |\n",
    "| all-mpnet-base-v2 | GPT-4.1 | 1.2035 | 54.55% | 0.7404 |\n",
    "| all-mpnet-base-v2 | CodeLlama | 1.2334 | 54.55% | 0.8506 |\n",
    "| all-mpnet-base-v2 | Claude 3.5 | 1.2502 | 51.52% | 0.9409 |\n",
    "\n",
    "##### Key Findings:\n",
    "\n",
    "**1. Embedding Model Performance:**\n",
    "- **all-MiniLM-L6-v2 consistently outperforms all-mpnet-base-v2** across all LLM combinations\n",
    "- Average distance improvement: ~0.07-0.11 points better with MiniLM\n",
    "- This pattern holds regardless of which LLM generates the queries\n",
    "\n",
    "**2. LLM Query Generation Quality:**\n",
    "- **GPT-4.1 generates the highest quality queries** (lowest distances)\n",
    "- **CodeLlama has the best query coverage** but with slightly higher distances\n",
    "- **Claude 3.5 shows the most variation** and generally higher distances\n",
    "\n",
    "**3. Best Combinations:**\n",
    "1. **all-MiniLM-L6-v2 + GPT-4.1** - Best overall performance\n",
    "2. **all-MiniLM-L6-v2 + CodeLlama** - Best coverage with good performance\n",
    "3. **all-mpnet-base-v2 + GPT-4.1** - Best MPNet combination\n",
    "\n",
    "##### Conclusion:\n",
    "\n",
    "The results consistently confirm that **all-MiniLM-L6-v2 is indeed performing better than all-mpnet-base-v2** for this specific loyalty points codebase. This is a genuine domain-specific finding that contradicts the general benchmark superiority of MPNet.\n",
    "\n",
    "**Winner: all-MiniLM-L6-v2 with GPT-4.1**\n",
    "- Lowest average retrieval distance (1.1287)\n",
    "- Best individual query performance (0.6485)\n",
    "- Most reliable semantic matching for this codebase"
   ],
   "id": "18c36ee4601b6e88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T14:44:51.753396Z",
     "start_time": "2025-07-10T14:44:51.654998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict, Any\n",
    "import json\n",
    "from rag.report_utils import generate_test_report\n",
    "\n",
    "def report_and_save(test_results: Dict[str, Any], output_file: str = \"results/results.json\"):\n",
    "    \"\"\" Generate and save report\n",
    "        Save test results to file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "    print(f\"Test results saved to: {output_file}\")\n",
    "\n",
    "    # Also save readable report\n",
    "    report = generate_test_report(test_results)\n",
    "    report_path = output_file.replace('.json', '_report.txt')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"Test report saved to: {report_path}\")"
   ],
   "id": "a741bcabef6e16e2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from vectorization.semantic_vector_database import SemanticVectorDatabase\n",
    "from rag.rag_tester import RAGTester\n",
    "\n",
    "vector_results = \"results/vectorization\"\n",
    "rag_results = \"results/rag\"\n",
    "\n",
    "chroma_base = \"{model}/chroma_db\"\n",
    "\n",
    "loyalty_collection = \"loyalty_code_semantics_{llm}\"\n",
    "models = [\n",
    "            { \"model\": \"all-MiniLM-L6-v2\", \"llms\": [\"claude3.5\", \"codellama\", \"gpt4.1\"] },\n",
    "            { \"model\": \"all-mpnet-base-v2\", \"llms\": [ \"claude3.5\", \"codellama\", \"gpt4.1\" ] }\n",
    "         ]\n",
    "\n",
    "rag_testers = []\n",
    "reports = []\n",
    "for model in models:\n",
    "    embedding_model = model[\"model\"]\n",
    "    print(\"embedding_model: \", embedding_model)\n",
    "    db_path = f\"{vector_results}/{chroma_base.format(model=embedding_model)}\"\n",
    "    print(\"db_path: \", db_path)\n",
    "\n",
    "    for llm in model[\"llms\"]:\n",
    "        print(llm)\n",
    "        collection_name = loyalty_collection.format(llm=llm)\n",
    "\n",
    "        vector_db = SemanticVectorDatabase(db_path, embedding_model)\n",
    "        rag_tester = RAGTester(vector_db)\n",
    "\n",
    "        test_results = rag_tester.run_test_suite(collection_name)\n",
    "        report_and_save(test_results, f\"{rag_results}/{embedding_model}.{collection_name}.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Interactive Testing:\n",
    "- Real-time query testing\n",
    "- Custom query exploration\n",
    "- Distance feedback per vector collection"
   ],
   "id": "80efa7d7524b169d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Quick interactive testing\n",
    "\n",
    "def run_test(query):\n",
    "    for model in models:\n",
    "        embedding_model = model[\"model\"]\n",
    "        db_path = chroma_base.format(model=embedding_model)\n",
    "\n",
    "        for llm  in model[\"llms\"]:\n",
    "            vector_db = SemanticVectorDatabase(db_path, embedding_model)\n",
    "\n",
    "            collection_name = loyalty_collection.format(llm=llm)\n",
    "            collection = vector_db.get_collection(collection_name)\n",
    "\n",
    "            result = collection.semantic_search(query, n_results=3)\n",
    "            print(f\"{embedding_model}-{llm}: Retrieved {result['summary']['total_results']} results\")\n",
    "            print(f\"{embedding_model}-{llm}: Average distance: {result['summary'].get('avg_distance', 0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERACTIVE TESTING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Try some custom queries (type 'quit' to exit):\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = input(\"\\nEnter your query: \").strip()\n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "\n",
    "        if query:\n",
    "            run_test(query)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ],
   "id": "1bb8897859a96d9c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
