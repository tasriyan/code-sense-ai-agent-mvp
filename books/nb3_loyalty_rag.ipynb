{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### What we are testing here:\n",
    "- Basic Semantic Retrieval Testing: Query testing with loyalty-specific queries\n",
    "- Distance analysis for semantic similarity quality\n",
    "- Result ranking and relevance assessment\n",
    "- File and project coverage analysis\n",
    "- Filtered Retrieval Testing: semantic + filtering by project type, or file type, or project name\n",
    "- Edge cases: malformed queries, empty queries, very long queries, etc.\n",
    "    Examples:\n",
    "    - Basic queries (loyalty points microservice specific):\n",
    "        - \"loyalty points calculation rules\"\n",
    "        - \"order processing workflow\"\n",
    "        - \"customer data integration\"\n",
    "        - \"payment service integration\"\n",
    "        - \"business rule patterns\"\n",
    "    - Filtered Queries:\n",
    "        - C# files only\n",
    "        - Configuration files only\n",
    "        - Specific project filtering\n",
    "#### Reporting:\n",
    "- Human readable reports to use to compare embedding models and classifier performance\n",
    "For example, the reports can be used for this type of analysis:\n",
    "##### Updated Performance Analysis\n",
    "\n",
    "| Embedding Model | LLM | Avg Distance | Query Coverage | Best Query Distance |\n",
    "|----------------|-----|-------------|----------------|-------------------|\n",
    "| **all-MiniLM-L6-v2** | **GPT-4.1** | **1.1287** | 54.55% | **0.6485** |\n",
    "| all-MiniLM-L6-v2 | CodeLlama | 1.1854 | **63.64%** | 0.7035 |\n",
    "| all-MiniLM-L6-v2 | Claude 3.5 | 1.1859 | 42.42% | 0.8229 |\n",
    "| all-mpnet-base-v2 | GPT-4.1 | 1.2035 | 54.55% | 0.7404 |\n",
    "| all-mpnet-base-v2 | CodeLlama | 1.2334 | 54.55% | 0.8506 |\n",
    "| all-mpnet-base-v2 | Claude 3.5 | 1.2502 | 51.52% | 0.9409 |\n",
    "\n",
    "##### Key Findings:\n",
    "\n",
    "**1. Embedding Model Performance:**\n",
    "- **all-MiniLM-L6-v2 consistently outperforms all-mpnet-base-v2** across all LLM combinations\n",
    "- Average distance improvement: ~0.07-0.11 points better with MiniLM\n",
    "- This pattern holds regardless of which LLM generates the queries\n",
    "\n",
    "**2. LLM Query Generation Quality:**\n",
    "- **GPT-4.1 generates the highest quality queries** (lowest distances)\n",
    "- **CodeLlama has the best query coverage** but with slightly higher distances\n",
    "- **Claude 3.5 shows the most variation** and generally higher distances\n",
    "\n",
    "**3. Best Combinations:**\n",
    "1. **all-MiniLM-L6-v2 + GPT-4.1** - Best overall performance\n",
    "2. **all-MiniLM-L6-v2 + CodeLlama** - Best coverage with good performance\n",
    "3. **all-mpnet-base-v2 + GPT-4.1** - Best MPNet combination\n",
    "\n",
    "##### Conclusion:\n",
    "\n",
    "The results consistently confirm that **all-MiniLM-L6-v2 is indeed performing better than all-mpnet-base-v2** for this specific loyalty points codebase. This is a genuine domain-specific finding that contradicts the general benchmark superiority of MPNet.\n",
    "\n",
    "**Winner: all-MiniLM-L6-v2 with GPT-4.1**\n",
    "- Lowest average retrieval distance (1.1287)\n",
    "- Best individual query performance (0.6485)\n",
    "- Most reliable semantic matching for this codebase"
   ],
   "id": "18c36ee4601b6e88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T04:07:13.471787Z",
     "start_time": "2025-07-12T04:07:13.464888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "from rag.report_utils import generate_test_report\n",
    "from rag.rag_report_analysis import RAGReportAnalyzer\n",
    "\n",
    "def report_and_save(test_results: Dict[str, Any], output_file: str = \"results/results.json\"):\n",
    "    \"\"\" Generate and save report\n",
    "        Save test results to file\n",
    "    \"\"\"\n",
    "\n",
    "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "    print(f\"Test results saved to: {output_file}\")\n",
    "\n",
    "    # Also save readable report\n",
    "    report = generate_test_report(test_results)\n",
    "    report_path = output_file.replace('.json', '_report.txt')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"Test report saved to: {report_path}\")\n",
    "\n",
    "\n",
    "def show_performance_analysis(reports_dir: str):\n",
    "    \"\"\"Example usage of the RAG Report Analyzer.\"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = RAGReportAnalyzer(reports_dir)\n",
    "\n",
    "    # Load reports\n",
    "    print(\"Loading RAG reports...\")\n",
    "    reports = analyzer.load_reports(\"*report*.txt\")\n",
    "\n",
    "    if not reports:\n",
    "        print(\"No reports found. Please ensure report files are in the current directory.\")\n",
    "        return\n",
    "\n",
    "    # Generate analysis\n",
    "    analyzer.print_analysis_summary()\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = analyzer.generate_performance_table()\n",
    "    if not df.empty:\n",
    "        df.to_csv(f'{reports_dir}/rag_performance_analysis.csv', index=False)\n",
    "        print(f\"\\nResults saved to 'rag_performance_analysis.csv'\")"
   ],
   "id": "a741bcabef6e16e2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T04:02:26.897775Z",
     "start_time": "2025-07-12T04:02:26.893888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_results = \"results/vectorization\"\n",
    "rag_results = \"results/rag\"\n",
    "\n",
    "chroma_base = \"{model}/loyalty_points_kb\"\n",
    "\n",
    "loyalty_collection = \"loyalty_code_semantics_{llm}\"\n",
    "models = [\n",
    "            { \"model\": \"all-MiniLM-L6-v2\", \"llms\": [\"claude3.5\", \"codellama\", \"gpt4.1\"] },\n",
    "            { \"model\": \"all-mpnet-base-v2\", \"llms\": [ \"claude3.5\", \"codellama\", \"gpt4.1\" ] }\n",
    "         ]"
   ],
   "id": "52c5db6f9dd61794",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from vectorization.semantic_vector_database import SemanticVectorDatabase\n",
    "from rag.rag_tester import RAGTester\n",
    "\n",
    "rag_testers = []\n",
    "reports = []\n",
    "for model in models:\n",
    "    embedding_model = model[\"model\"]\n",
    "    print(\"embedding_model: \", embedding_model)\n",
    "    db_path = f\"{vector_results}/{chroma_base.format(model=embedding_model)}\"\n",
    "    print(\"db_path: \", db_path)\n",
    "\n",
    "    for llm in model[\"llms\"]:\n",
    "        print(llm)\n",
    "        collection_name = loyalty_collection.format(llm=llm)\n",
    "\n",
    "        vector_db = SemanticVectorDatabase(db_path, embedding_model)\n",
    "        rag_tester = RAGTester(vector_db)\n",
    "\n",
    "        test_results = rag_tester.run_test_suite(collection_name)\n",
    "        report_and_save(test_results, f\"{rag_results}/{embedding_model}.{collection_name}.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T04:07:18.534021Z",
     "start_time": "2025-07-12T04:07:18.513566Z"
    }
   },
   "cell_type": "code",
   "source": "show_performance_analysis(rag_results)",
   "id": "beb3027c772f60a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAG reports...\n",
      "============================================================\n",
      "RAG PERFORMANCE ANALYSIS SUMMARY\n",
      "============================================================\n",
      "Total Reports Analyzed: 6\n",
      "Embedding Models: 3\n",
      "LLM Models: 3\n",
      "\n",
      "PERFORMANCE COMPARISON TABLE:\n",
      "| Embedding Model   | LLM       |   Avg Distance |   Query Coverage (%) |   Best Query Distance |\n",
      "|:------------------|:----------|---------------:|---------------------:|----------------------:|\n",
      "| ** all-MiniLM-L6-v2  ** | ** gpt4.1    ** | **         1.1296 ** | **              51.5200 ** | **                0.6464 ** |\n",
      "| all-MiniLM-L6-v2  | claude3.5 |         1.1699 |              51.5200 |                0.7950 |\n",
      "| unknown           | codellama |         1.1797 |              60.6100 |                0.7286 |\n",
      "| all-mpnet-base-v2 | gpt4.1    |         1.1933 |              63.6400 |                0.7520 |\n",
      "| unknown           | codellama |         1.2374 |              48.4800 |                0.8422 |\n",
      "| all-mpnet-base-v2 | claude3.5 |         1.2389 |              57.5800 |                0.9144 |\n",
      "\n",
      "KEY FINDINGS:\n",
      "• Best Overall Performance: all-MiniLM-L6-v2 + gpt4.1 (Avg Distance: 1.1296)\n",
      "• Best Query Coverage: all-mpnet-base-v2 + gpt4.1 (63.64%)\n",
      "• Best Individual Query: all-MiniLM-L6-v2 + gpt4.1 (Distance: 0.6464)\n",
      "\n",
      "EMBEDDING MODEL COMPARISON:\n",
      "                   avg_distance  query_coverage  best_query_distance\n",
      "embedding_model                                                     \n",
      "all-MiniLM-L6-v2         1.1498          51.520               0.7207\n",
      "all-mpnet-base-v2        1.2161          60.610               0.8332\n",
      "unknown                  1.2086          54.545               0.7854\n",
      "\n",
      "LLM MODEL COMPARISON:\n",
      "           avg_distance  query_coverage  best_query_distance\n",
      "llm_model                                                   \n",
      "claude3.5        1.2044          54.550               0.8547\n",
      "codellama        1.2086          54.545               0.7854\n",
      "gpt4.1           1.1614          57.580               0.6992\n",
      "\n",
      "Results saved to 'rag_performance_analysis.csv'\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Interactive Testing:\n",
    "- Real-time query testing\n",
    "- Custom query exploration\n",
    "- Distance feedback per vector collection"
   ],
   "id": "80efa7d7524b169d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Quick interactive testing\n",
    "\n",
    "def run_test(query):\n",
    "    for model in models:\n",
    "        embedding_model = model[\"model\"]\n",
    "        db_path = chroma_base.format(model=embedding_model)\n",
    "\n",
    "        for llm  in model[\"llms\"]:\n",
    "            vector_db = SemanticVectorDatabase(db_path, embedding_model)\n",
    "\n",
    "            collection_name = loyalty_collection.format(llm=llm)\n",
    "            collection = vector_db.get_collection(collection_name)\n",
    "\n",
    "            result = collection.semantic_search(query, n_results=3)\n",
    "            print(f\"{embedding_model}-{llm}: Retrieved {result['summary']['total_results']} results\")\n",
    "            print(f\"{embedding_model}-{llm}: Average distance: {result['summary'].get('avg_distance', 0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERACTIVE TESTING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Try some custom queries (type 'quit' to exit):\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = input(\"\\nEnter your query: \").strip()\n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "\n",
    "        if query:\n",
    "            run_test(query)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ],
   "id": "1bb8897859a96d9c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
