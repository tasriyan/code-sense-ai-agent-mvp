{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### What we are testing here:\n",
    "- Basic Semantic Retrieval Testing: Query testing with loyalty-specific queries\n",
    "- Distance analysis for semantic similarity quality\n",
    "- Result ranking and relevance assessment\n",
    "- File and project coverage analysis\n",
    "- Filtered Retrieval Testing: semantic + filtering by project type, or file type, or project name\n",
    "- Edge cases: malformed queries, empty queries, very long queries, etc.\n",
    "    Examples:\n",
    "    - Basic queries (loyalty points microservice specific):\n",
    "        - \"loyalty points calculation rules\"\n",
    "        - \"order processing workflow\"\n",
    "        - \"customer data integration\"\n",
    "        - \"payment service integration\"\n",
    "        - \"business rule patterns\"\n",
    "    - Filtered Queries:\n",
    "        - C# files only\n",
    "        - Configuration files only\n",
    "        - Specific project filtering\n",
    "#### Reporting:\n",
    "- Human readable reports to use to compare embedding models and classifier performance\n",
    "For example, the reports can be used for this type of analysis:\n",
    "##### Updated Performance Analysis\n",
    "\n",
    "| Embedding Model | LLM | Avg Distance | Query Coverage | Best Query Distance |\n",
    "|----------------|-----|-------------|----------------|-------------------|\n",
    "| **all-MiniLM-L6-v2** | **GPT-4.1** | **1.1287** | 54.55% | **0.6485** |\n",
    "| all-MiniLM-L6-v2 | CodeLlama | 1.1854 | **63.64%** | 0.7035 |\n",
    "| all-MiniLM-L6-v2 | Claude 3.5 | 1.1859 | 42.42% | 0.8229 |\n",
    "| all-mpnet-base-v2 | GPT-4.1 | 1.2035 | 54.55% | 0.7404 |\n",
    "| all-mpnet-base-v2 | CodeLlama | 1.2334 | 54.55% | 0.8506 |\n",
    "| all-mpnet-base-v2 | Claude 3.5 | 1.2502 | 51.52% | 0.9409 |\n",
    "\n",
    "##### Key Findings:\n",
    "\n",
    "**1. Embedding Model Performance:**\n",
    "- **all-MiniLM-L6-v2 consistently outperforms all-mpnet-base-v2** across all LLM combinations\n",
    "- Average distance improvement: ~0.07-0.11 points better with MiniLM\n",
    "- This pattern holds regardless of which LLM generates the queries\n",
    "\n",
    "**2. LLM Query Generation Quality:**\n",
    "- **GPT-4.1 generates the highest quality queries** (lowest distances)\n",
    "- **CodeLlama has the best query coverage** but with slightly higher distances\n",
    "- **Claude 3.5 shows the most variation** and generally higher distances\n",
    "\n",
    "**3. Best Combinations:**\n",
    "1. **all-MiniLM-L6-v2 + GPT-4.1** - Best overall performance\n",
    "2. **all-MiniLM-L6-v2 + CodeLlama** - Best coverage with good performance\n",
    "3. **all-mpnet-base-v2 + GPT-4.1** - Best MPNet combination\n",
    "\n",
    "##### Conclusion:\n",
    "\n",
    "The results consistently confirm that **all-MiniLM-L6-v2 is indeed performing better than all-mpnet-base-v2** for this specific loyalty points codebase. This is a genuine domain-specific finding that contradicts the general benchmark superiority of MPNet.\n",
    "\n",
    "**Winner: all-MiniLM-L6-v2 with GPT-4.1**\n",
    "- Lowest average retrieval distance (1.1287)\n",
    "- Best individual query performance (0.6485)\n",
    "- Most reliable semantic matching for this codebase"
   ],
   "id": "18c36ee4601b6e88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:51:50.649999Z",
     "start_time": "2025-07-14T02:51:49.808349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from vectorization.semantic_match import SemanticMatch\n",
    "from datetime import datetime\n",
    "\n",
    "from rag.report_utils import calculate_performance_metrics\n",
    "from vectorization.semantic_vector_database import SemanticVectorDatabase\n",
    "\n",
    "def run_test_suite(vector_db: SemanticVectorDatabase,  collection_name: str):\n",
    "    \"\"\"Run comprehensive RAG test suite\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPREHENSIVE RAG TEST SUITE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    collection = vector_db.get_collection(collection_name)\n",
    "\n",
    "    test_results: Dict[str, List[SemanticMatch]] = {\n",
    "        'basic_tests': [],\n",
    "        'filtered_tests': [],\n",
    "        'edge_case_tests': [],\n",
    "    }\n",
    "\n",
    "    # Test 1: Basic semantic queries\n",
    "    print(\"\\n1. BASIC SEMANTIC RETRIEVAL TESTS\")\n",
    "    # Query testing\n",
    "    # Distance analysis for semantic similarity quality\n",
    "    # Result ranking and relevance assessment\n",
    "    # File and project coverage analysis\n",
    "\n",
    "    basic_queries = [\n",
    "        \"loyalty points calculation rules\",\n",
    "        \"order processing workflow\",\n",
    "        \"customer data integration\",\n",
    "        \"payment service integration\",\n",
    "        \"business rule patterns\",\n",
    "        \"event handlers\",\n",
    "        \"database operations\",\n",
    "        \"service dependencies\",\n",
    "        \"configuration settings\",\n",
    "        \"loyalty point rewards\"\n",
    "    ]\n",
    "\n",
    "    for query in basic_queries:\n",
    "        result = collection.semantic_search(query, n_results=3)\n",
    "        test_results['basic_tests'].append(result)\n",
    "\n",
    "    # Test 2: Filtered retrieval\n",
    "    print(\"\\n2. FILTERED RETRIEVAL TESTS\")\n",
    "    # Metadata filtering by file type, project, etc.\n",
    "    # Filter effectiveness measurement\n",
    "\n",
    "    filtered_tests = [\n",
    "        {\n",
    "            'query': \"loyalty points calculation\",\n",
    "            'filters': {'file_type': 'cs'},\n",
    "            'description': 'C# files only'\n",
    "        },\n",
    "        {\n",
    "            'query': \"service integration\",\n",
    "            'filters': {'file_type': 'appsettings'},\n",
    "            'description': 'Configuration files only'\n",
    "        },\n",
    "        {\n",
    "            'query': \"business rules\",\n",
    "            'filters': {'project_name': 'LoyaltyPoints'},\n",
    "            'description': 'Main project only'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for test in filtered_tests:\n",
    "        print(f\"\\nTesting: {test['description']}\")\n",
    "        result = collection.filtered_semantic_search(\n",
    "            test['query'],\n",
    "            test['filters'],\n",
    "            n_results=3\n",
    "        )\n",
    "        test_results['filtered_tests'].append(result)\n",
    "\n",
    "    # Test 3: Edge cases\n",
    "    print(\"\\n3. EDGE CASE TESTS\")\n",
    "     # Empty queries and malformed input\n",
    "     # Non-existent terms for robustness\n",
    "     # Single character and stop words handling\n",
    "     # Very long queries for boundary testing\n",
    "\n",
    "    edge_cases = [\n",
    "        \"\",  # Empty query\n",
    "        \"xyzabc123nonexistent\",  # Non-existent terms\n",
    "        \"a\",  # Single character\n",
    "        \"the and or but\",  # Stop words only\n",
    "        \"loyalty\" * 50,  # Very long query\n",
    "    ]\n",
    "\n",
    "    for query in edge_cases:\n",
    "        result = collection.semantic_search(query, n_results=1)\n",
    "        test_results['edge_case_tests'].append(result)\n",
    "\n",
    "    # Test 4: Performance metrics\n",
    "    print(\"\\n4. PERFORMANCE METRICS\")\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    collection_stats = collection.get_collection_stats_v2()\n",
    "    performance_metrics = calculate_performance_metrics(test_results, collection_stats)\n",
    "\n",
    "    return timestamp, performance_metrics, test_results, collection_stats"
   ],
   "id": "da4f059a3a0c5bca",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:51:56.526699Z",
     "start_time": "2025-07-14T02:51:56.522268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "from rag.report_utils import generate_rag_report\n",
    "\n",
    "def report_and_save(test_results: Dict[str, Any], output_file: str = \"results/results.json\"):\n",
    "    \"\"\" Generate and save report\n",
    "        Save test results to file\n",
    "    \"\"\"\n",
    "\n",
    "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "    print(f\"Test results saved to: {output_file}\")\n",
    "\n",
    "    # Also save readable report\n",
    "    report = generate_rag_report(test_results)\n",
    "    report_path = output_file.replace('.json', '_report.txt')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"Test report saved to: {report_path}\")"
   ],
   "id": "a741bcabef6e16e2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:52:00.453548Z",
     "start_time": "2025-07-14T02:52:00.448179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_results = \"results/vectorization\"\n",
    "rag_results = \"results/rag\"\n",
    "\n",
    "chroma_base = \"{model}/loyalty_points_kb\"\n",
    "\n",
    "loyalty_collection = \"loyalty_code_semantics_{llm}\"\n",
    "models = [\n",
    "            { \"model\": \"all-MiniLM-L6-v2\", \"llms\": [ \"claude3.5\", \"claude3.7\", \"claude4.0\", \"codellama\", \"gpt4.1\"] },\n",
    "            { \"model\": \"all-mpnet-base-v2\", \"llms\": [ \"claude3.5\", \"claude3.7\", \"claude4.0\", \"codellama\", \"gpt4.1\" ] }\n",
    "         ]"
   ],
   "id": "52c5db6f9dd61794",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from vectorization.semantic_vector_database import SemanticVectorDatabase\n",
    "\n",
    "rag_testers = []\n",
    "reports = []\n",
    "for model in models:\n",
    "    embedding_model = model[\"model\"]\n",
    "    print(\"embedding_model: \", embedding_model)\n",
    "    db_path = f\"{vector_results}/{chroma_base.format(model=embedding_model)}\"\n",
    "    print(\"db_path: \", db_path)\n",
    "\n",
    "    for llm in model[\"llms\"]:\n",
    "        print(llm)\n",
    "        collection_name = loyalty_collection.format(llm=llm)\n",
    "\n",
    "        vector_db = SemanticVectorDatabase(db_path, embedding_model)\n",
    "        timestamp, performance_metrics, test_results, collection_stats = run_test_suite(vector_db, collection_name)\n",
    "        output: Dict[str, Any] = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"performance_metrics\": performance_metrics,\n",
    "            \"collection_stats\": collection_stats,\n",
    "        }\n",
    "        output.update(test_results)\n",
    "        report_and_save(output, f\"{rag_results}/{embedding_model}.{collection_name}.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:52:27.672746Z",
     "start_time": "2025-07-14T02:52:27.535295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rag.analyzer import RAGReportAnalyzer\n",
    "\n",
    "def show_performance_analysis(reports_dir: str):\n",
    "    \"\"\"Example usage of the RAG Report Analyzer.\"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = RAGReportAnalyzer(reports_dir)\n",
    "\n",
    "    # Load reports\n",
    "    print(\"Loading RAG reports...\")\n",
    "    reports = analyzer.load_reports(\"*report*.txt\")\n",
    "\n",
    "    if not reports:\n",
    "        print(\"No reports found. Please ensure report files are in the current directory.\")\n",
    "        return\n",
    "\n",
    "    # Generate analysis\n",
    "    analyzer.print_analysis_summary()\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = analyzer.generate_performance_table()\n",
    "    if not df.empty:\n",
    "        df.to_csv(f'{reports_dir}/rag_performance_analysis.csv', index=False)\n",
    "        print(f\"\\nResults saved to 'rag_performance_analysis.csv'\")\n",
    "\n",
    "show_performance_analysis(rag_results)"
   ],
   "id": "beb3027c772f60a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAG reports...\n",
      "============================================================\n",
      "RAG PERFORMANCE ANALYSIS SUMMARY\n",
      "============================================================\n",
      "Total Reports Analyzed: 10\n",
      "Embedding Models: 2\n",
      "LLM Models: 5\n",
      "\n",
      "PERFORMANCE COMPARISON TABLE:\n",
      "| Embedding Model   | LLM       |   Avg Distance |   Query Coverage (%) |   Best Query Distance |\n",
      "|:------------------|:----------|---------------:|---------------------:|----------------------:|\n",
      "| ** all-MiniLM-L6-v2  ** | ** claude4.0 ** | **         1.1268 ** | **              54.5500 ** | **                0.7034 ** |\n",
      "| all-MiniLM-L6-v2  | gpt4.1    |         1.1296 |              51.5200 |                0.6464 |\n",
      "| all-MiniLM-L6-v2  | claude3.7 |         1.1399 |              54.5500 |                0.7430 |\n",
      "| all-MiniLM-L6-v2  | claude3.5 |         1.1699 |              51.5200 |                0.7950 |\n",
      "| all-MiniLM-L6-v2  | codellama |         1.1797 |              60.6100 |                0.7286 |\n",
      "| all-mpnet-base-v2 | gpt4.1    |         1.1933 |              63.6400 |                0.7520 |\n",
      "| all-mpnet-base-v2 | claude3.7 |         1.1975 |              57.5800 |                0.9097 |\n",
      "| all-mpnet-base-v2 | claude4.0 |         1.2042 |              48.4800 |                0.7944 |\n",
      "| all-mpnet-base-v2 | codellama |         1.2374 |              48.4800 |                0.8422 |\n",
      "| all-mpnet-base-v2 | claude3.5 |         1.2389 |              57.5800 |                0.9144 |\n",
      "\n",
      "KEY FINDINGS:\n",
      "• Best Overall Performance: all-MiniLM-L6-v2 + claude4.0 (Avg Distance: 1.1268)\n",
      "• Best Query Coverage: all-mpnet-base-v2 + gpt4.1 (63.64%)\n",
      "• Best Individual Query: all-MiniLM-L6-v2 + gpt4.1 (Distance: 0.6464)\n",
      "\n",
      "EMBEDDING MODEL COMPARISON:\n",
      "                   avg_distance  query_coverage  best_query_distance\n",
      "embedding_model                                                     \n",
      "all-MiniLM-L6-v2         1.1492          54.550               0.7233\n",
      "all-mpnet-base-v2        1.2143          55.152               0.8425\n",
      "\n",
      "LLM MODEL COMPARISON:\n",
      "           avg_distance  query_coverage  best_query_distance\n",
      "llm_model                                                   \n",
      "claude3.5        1.2044          54.550               0.8547\n",
      "claude3.7        1.1687          56.065               0.8264\n",
      "claude4.0        1.1655          51.515               0.7489\n",
      "codellama        1.2086          54.545               0.7854\n",
      "gpt4.1           1.1614          57.580               0.6992\n",
      "\n",
      "Results saved to 'rag_performance_analysis.csv'\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Interactive Testing:\n",
    "- Real-time query testing\n",
    "- Custom query exploration\n",
    "- Distance feedback per vector collection"
   ],
   "id": "80efa7d7524b169d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Quick interactive testing\n",
    "\n",
    "def run_test(query):\n",
    "    for model in models:\n",
    "        embedding_model = model[\"model\"]\n",
    "        db_path = chroma_base.format(model=embedding_model)\n",
    "\n",
    "        for llm  in model[\"llms\"]:\n",
    "            vector_db = SemanticVectorDatabase(db_path, embedding_model)\n",
    "\n",
    "            collection_name = loyalty_collection.format(llm=llm)\n",
    "            collection = vector_db.get_collection(collection_name)\n",
    "\n",
    "            result = collection.semantic_search(query, n_results=3)\n",
    "            print(f\"{embedding_model}-{llm}: Retrieved {result['summary']['total_results']} results\")\n",
    "            print(f\"{embedding_model}-{llm}: Average distance: {result['summary'].get('avg_distance', 0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERACTIVE TESTING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Try some custom queries (type 'quit' to exit):\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = input(\"\\nEnter your query: \").strip()\n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "\n",
    "        if query:\n",
    "            run_test(query)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ],
   "id": "1bb8897859a96d9c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
